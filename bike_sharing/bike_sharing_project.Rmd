---
title: "Bike Sharing Project"
author: Carme Mias
date: June 2020
output: html_notebook
---

Project exercise from the "Data Science and machine Learning bootcamp with R" course (section 21, Linear Regression) and using the data from the [Bike Sharing](https://www.kaggle.com/c/bike-sharing-demand/overview) Kaggle competition.

Data available to download from the Kaggle website.

```{r required-libraries, include=FALSE, eval=TRUE}
install.packages(c(ggplot2, corrplot, corrgram))
require('ggplot2')
require('corrplot')
require('corrgram')
```

## Get the Data

```{r initial-data-frames}
training_data <- read.csv('data/train.csv')
test_data <- read.csv('data/test.csv')

head(training_data)
summary(training_data)
nrow(training_data)
```

The variable we are trying to predict is `count`.

## Data Exploration

First, a temperature/count scatterplot:

```{r temperature-count-plot}
ggplot( training_data, aes( x=temp, y=count, color=temp ) ) + 
  geom_point( alpha=0.3 ) + scale_color_gradient(low = 'blue', high = 'orange')  +
  xlab( 'Temperature, Celsius' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Temperature/Count scatterplot' )
```

Then, a datetime/count scatterplot:

```{r datetime-count-scatterplot}
ggplot( training_data, aes( x=as.POSIXct(datetime), y=count, color=temp ) ) + 
  geom_point( alpha=0.3 ) + scale_color_gradient(low = 'blue', high = 'orange')  +
  xlab( 'Date and time' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Datetime/Count scatterplot' )
```
From the graph, we can conclude that, in general, the number of bicycles rented has been growing steadily and that there are more rentals when the temperature is high than when it's low.

## The Linear Regression Model

```{r temp-count-correlation}
temp_count_cor <- cor(training_data[,c('temp', 'count')])
print(temp_count_cor)
```

```{r season-count-boxplot}
ggplot(training_data, aes(x=as.factor(season), y=count, color = as.factor(season))) +
  geom_boxplot() + scale_x_discrete(labels=c("1"="Spring (1)", "2"="Summer (2)", "3"="Autumn (3)", "4"="Winter (4)")) +
  xlab( 'Season' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Season/Count boxplot' )
```

Two characteritics stand out: one is that there are a lot of outlying values, the other is that there seemed to have been more bicycles rented in winter than in spring.

## Feature Engineering

We'll separate the time from the date and have it in a separate column called *time*.

```{r time-column}
time <- as.POSIXlt(training_data[,'datetime'])$hour
training_data <- cbind(training_data, time)
head(training_data)
```

Now we can see the bike use variations according to time of the day:

```{r by-time-for-working-days}
ggplot(subset(training_data, workingday == 1), aes(x=time, y=count, color=temp)) + 
  geom_point(size = 2, alpha=0.3, position=position_jitter(w=0.35, h=0)) + scale_color_gradient(low = 'blue', high = 'orange')  +
  xlab( 'Time of Day' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Time of Day/Count on Working Days' )
```

```{r by-time-for-non-working-days}
ggplot(subset(training_data, workingday == 0), aes(x=time, y=count, color=temp)) + 
  geom_point(size = 2, alpha=0.3, position=position_jitter(w=0.35, h=0)) + scale_color_gradient(low = 'blue', high = 'orange')  +
  xlab( 'Time of Day' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Time of Day/Count on Non-Working Days' )
```

## Build the Model

We'll first build an initial linear model using only one variable:

```{r linear-model-temperature}
temp_model <- lm(formula = count ~ temp, data = training_data)
summary(temp_model)
```
Although the p-value is low, the R Squared value is also very low, so this model is not a good fit.

How may bike rentals does this model predict when the temperature is 25 degrees Celsius?

```{r prediction}
temp.25 <- data.frame(temp=c(25))
print(predict(temp_model, temp.25))
```

Let's now use more variables to build the linear regression model:

```{r linear-model-multiple-vars}
multiple_var_model <- lm(formula = count ~ season + holiday + workingday + weather + temp + humidity + windspeed + time , data = training_data)
summary(multiple_var_model)
```

Better but still not great. We can also see what the residuals don't look like a normal distribution:

```{r model-residuals}
res <- residuals(multiple_var_model)
res <- as.data.frame(res)

ggplot(res, aes(res)) + geom_histogram(fill='blue') + 
  xlab( 'Residual' ) + ylab( 'Number of bikes rented' ) + ggtitle( 'Residual/Count histogram' )
```

We could also check what predictions this model would make using the test data:

```{r prepare-test-data}
time <- as.POSIXlt(test_data[,'datetime'], format="%Y-%m-%d %H:%M:%S")$hour
test_data <- cbind(test_data, time)
head(test_data)
```

```{r predictions}
bike_count_predictions <- predict(multiple_var_model, test_data)
```

We can't calculate the mean squared error because the test data does not have the count.